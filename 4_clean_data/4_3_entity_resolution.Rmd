---
title: "Link pids referring to the same child"
output:
  html_document:
    df_print: paged
---

## 1. Prepare packages and data
```{r load package and read data}
library(ArrowDQAToolkit)
Sys.setenv(SPARK_HOME="/home/anhptq/spark/spark-3.3.0-bin-hadoop3")

personal_info <- as_arrow_table(read_parquet(file.path("/./", "cluster_data", "vrdata", "standardized", "personal_info.parquet")))
personal_info
```

## 2. Grouping possible duplicated entities by blocking

Try blocking based on 3 attributes: \
- date of birth (dob).  
- ethnic.  
- gender.   

Only perform entity resolution on data where name or caregiver is valid.
Definition for a valid name: name must have at least 2 characters.

Only perform entity resolution on data where min vacdate and max vacdate are different. (Assumption: an entity cannot be given multiple pids on the same date)

```{r blocking}
# get blocks that are valid for further entity resolution step
valid_blocks <- personal_info %>% 
  filter(
    str_length(name) > 1 | str_length(caregiver) > 1
  ) %>% 
  mutate(
    block = paste(dob, ethnic, sex, sep="_")
  ) %>% 
  group_by(block) %>% 
  summarize(
    no_records = n(),
    min_date = min(vacdate),
    max_date = max(vacdate)
  ) %>% 
  filter( (min_date != max_date) & (no_records > 1) ) %>% 
  compute()
valid_blocks
```

### Dataset with candidates for entity resolution 
```{r candidates for entity resolution}
candidates <- personal_info %>%  
  filter(
    # only include records with valid name of caregiver
    str_length(name) > 1 | str_length(caregiver) > 1
  ) %>% 
  mutate(
    block = paste(dob, ethnic, sex, sep="_")
  ) %>% 
  filter(block %in% valid_blocks$block) %>% 
  compute()

candidates
```

Initial dataset: 14,400,339 records
Candidates for entity resolution: 14,350,648 records

### Calculate total number of pairs
```{r total number of pairs for comparison}
valid_blocks %>% 
  mutate(
    block_pairs = no_records*(no_records - 1)/2
  ) %>% 
  summarize(
    total_pairs = sum(block_pairs)
  ) %>% 
  collect()
```
Number of candidate pairs: 14,283,883,282 -> need pruning to reduce this number

### Generate subset of data for testing

```{r generate subset of data for testing}
subset_blocks <- valid_blocks[1:10,]

subset_data <- candidates %>% 
  filter(block %in% subset_blocks$block) %>% 
  compute()

# -- test: check caregivers associated with more than 1 pid
subset_data %>% 
  group_by(caregiver) %>% 
  count() %>% 
  filter(n > 1) %>% 
  collect()

# --- check number of rows in the sampled dataset
# only test on dataset with more than 10k samples
subset_data %>% compute()
```

## 3. Test on subset of data

Some problem specific rules
Pair pruning
- *Same latest vacdate* -> not the same entity

Features for classification across candidate pairs 
- Only consider *name similarity* when name is not the same as caregiver name.  
- Only consider *caregiver similarity* when caregiver name is a valid name (at least 2 characters).   


```{python}
#Package versions:
#    - pandas 1.5.3
 #   - pyspark 3.3.0

# import splink utilities
import pyarrow as pa
import pandas as pd
import splink
from splink.spark.linker import SparkLinker
import splink.spark.comparison_library as cl
import splink.spark.comparison_template_library as ctl
from splink.spark.blocking_rule_library import block_on
from splink.spark.jar_location import similarity_jar_location
import altair_viewer
import altair as alt
# import spark utilities
from pyspark.sql import SparkSession, types
from pyspark.conf import SparkConf



# use 3.3 jar instead of 3.x
path = '/./home/anhptq/miniconda3/envs/vaccine_reg/lib/python3.9/site-packages/splink/files/spark_jars/scala-udf-similarity-0.1.1_spark3.x.jar'

spark_config = SparkConf()
spark_config.set("spark.jars", path)
spark_config.set("spark.executor.memory", "110g")
spark_config.set("spark.executor.core", "40")


spark = SparkSession.builder.config(conf = spark_config).getOrCreate()
spark.sparkContext.setCheckpointDir("./checkpoints")

# check spark version
spark.version
# making sure the path is registered
spark.conf.get("spark.jars")
spark.conf.get("spark.executor.memory")
```

Define linking configurations
```{python}
# convert data to pandas
subset_data = r.subset_data.to_pandas()

# --- Custom comparison logic for comparing names
# only compare names when name != caregiver
# in which case compare it the same way as comparing caregiver name
name_comparison = {
  "output_column_name": "name",
  "comparison_description": "Name comparison",
  "comparison_levels": [
    {
         # (when name == caregiver, treat name as missing)
        "sql_condition": "name_l  = caregiver_l OR name_r = caregiver_r",
        "label_for_charts": "Null",
        "is_null_level": True,
    },
    {
        "sql_condition": "name_l = name_r",
        "label_for_charts": "Exact match",
        "m_probability": 0.8,
        "u_probability": 0.1,
    },
    {
        "sql_condition": "jaro_winkler(name_l, name_r) >= 0.9",
        "label_for_charts": "Jaro Winkler similarity &gt; to 0.9",
    },
    {
        "sql_condition": "jaro_winkler(name_l, name_r) >= 0.8",
        "label_for_charts": "Jaro Winkler similarity &gt; 0.8",
    },
    {
        "sql_condition": "damerau_levenshtein(name_l, name_r) <= 1",
        "label_for_charts": "Damerau Levenshtein distance &lt; 1",
    },
    {
        "sql_condition": "damerau_levenshtein(name_l, name_r) <= 3",
        "label_for_charts": "Damerau Levenshtein distance &lt; 3",
    },
    {
        "sql_condition": "damerau_levenshtein(name_l, name_r) <= 5",
        "label_for_charts": "Damerau Levenshtein distance &lt; 5",
    },
    {"sql_condition": "ELSE", "label_for_charts": "All other comparisons"},
  ]
}

# --- Settings for computing comparison vector
settings = {
  "unique_id_column_name": "pid",
  "link_type":"dedupe_only", 
  "blocking_rules_to_generate_predictions": [
        block_on("block")
  ],
  "max_iterations":100,
  "retain_matching_columns": True, 
  "retain_intermediate_calculation_columns": True,
  "comparisons": [
      # set starting m value for exact match 
      name_comparison,
      ctl.name_comparison("caregiver", damerau_levenshtein_thresholds = [1, 3, 5], m_probability_exact_match_name = 0.7),
      cl.exact_match("fup"),
      # only check whether vacdate is the same or not
      cl.exact_match("vacdate")
      # cl.exact_match("duplicated")
  ]}
  
  
```

Export data frame to spark
```{python}
# export dataframe to spark
subset_data = spark.createDataFrame(subset_data)
```

Try detection with default settings, built-in comparison rule

```{python}
# --- Set up linker
linker = SparkLinker([subset_data], settings)

# --- Set up rule to estimate priors
# deterministic rule to estimate prior
deterministic_rule = [
  "l.name = r.name or l.caregiver = r.caregiver"
]
# guess recall of deterministic linking to be 0.4
linker.estimate_probability_two_random_records_match(deterministic_rule, recall = 0.4)

# --- Set up training for u value
linker.estimate_u_using_random_sampling(max_pairs=1e7)

# --- Set up training blocking rule for m value
training_block_rule = block_on(["dob", "ethnic", "sex"]) 
linker.estimate_parameters_using_expectation_maximisation(training_block_rule)

```
Try predicting result
```{python predict result with linker}
results = linker.predict(threshold_match_probability=0.5)
result_pd = results.as_pandas_dataframe()

pd.set_option('display.max_columns', 20)
result_pd[["pid_l","pid_r", "name_l", "name_r", "caregiver_l", "caregiver_r", "fup_l", "fup_r"]]
result_pd

```


```{python visualize model}
records_to_view = results.as_record_dict(limit=10)
linker.waterfall_chart(records_to_view)

estimate_parameter_chart = linker.parameter_estimate_comparisons_chart()
estimate

```


